# Voice Stack ASR Production Configuration Template
# Copy this file to the repository root as .env and customize as needed

# ---- Server ----
ENV=production
LOG_LEVEL=INFO
HOST=0.0.0.0
ASR_PORT=5001
CORS_ORIGINS=*

# ---- Debugpy (disable in production) ----
DEBUGPY_ENABLE=false

# ---- ASR ----
# Engine: fasterwhisper (recommended for production) or whisper
ASR_ENGINE=fasterwhisper

# Device: cuda (for NVIDIA GPU), cpu (for CPU-only), or auto (auto-detect)
# Change to cuda if you have NVIDIA GPU with CUDA support
ASR_DEVICE=cpu

# Model: tiny, base, small, medium, large-v1, large-v2, large-v3
# Larger models are more accurate but require more resources
# Recommended: medium for balanced accuracy/performance, large-v3 for best accuracy
ASR_MODEL=medium

# Model location: Where to cache downloaded models
# If not set, defaults to ~/.cache/huggingface/hub/
# ASR_MODEL_LOCATION=/path/to/models

# CPU Threads: Number of CPU threads for processing
# Adjust based on your CPU cores (typically 4-16)
ASR_CPU_THREADS=4

# Workers: Number of concurrent transcription workers
# Increase for higher concurrency (uses more memory)
ASR_NUM_OF_WORKERS=1

# Compute Type: Quantization level for models
# int8: Fastest, lowest memory, slightly reduced accuracy
# int8_float16: Balanced (recommended for CUDA)
# float16: Higher accuracy, more memory (CUDA only)
# float32: Highest accuracy, most memory (CPU or CUDA)
ASR_COMPUTE_TYPE=int8

# Cache: Enable response caching for repeated requests
ASR_CACHE_ENABLED=true
ASR_CACHE_MAX_ITEMS=100

# Transcription Settings
ASR_TRANSCRIBE_BEAM_SIZE=5
ASR_TRANSCRIBE_TEMPERATURE=0
ASR_TRANSCRIBE_BEST_OF=1

# VAD (Voice Activity Detection): Reduces processing time by 30-50%
# Highly recommended for production
ASR_VAD_ENABLED=true
ASR_VAD_THRESHOLD=0.5

# Language: Leave empty for auto-detection, or specify (e.g., en, es, fr)
# ASR_TRANSCRIBE_LANG=

# ---- Notes ----
# 1. For GPU support, ensure CUDA 12.1+ and appropriate drivers are installed
# 2. Run scripts/install_torch.sh to install PyTorch with CUDA support
# 3. Larger models (medium, large) require more RAM and VRAM
# 4. Enable VAD for better performance on audio with silence periods
# 5. Use fasterwhisper for 2-4x speedup over vanilla whisper
